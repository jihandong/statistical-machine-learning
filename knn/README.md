# K近邻（K Nearest Neighbors）

**“伪训练”**：实际上并没有做什么训练，也没有得到什么参数，只是把训练数据整理了一下，每次分类时带着整个训练集来判断（所以显然是只适合小数据集的，而且对数据集质量要求也很高）。

**kd树**：为了加速找到邻居的过程，使用了kd树（K Demensions Tree）的数据结构，这是一颗分割特征空间的平衡二叉树，每个节点都对应一个样本，代表着一个超平面（把特征空间切成两半）；通过用kd树分割特征空间，我们在找x的邻居时，就可以少查询很多空间中的样本（ $O(N)$降到 $O(lgN)$）

PS：kd树这种递归二分特征空间的思路在“决策树”一章也能见到，虽然目的不同，但思路是相似的。

PS：与感知机不同，KNN的实现很难运用向量化方法来加速，因为感知机、SVM等问题都很方便转化成矩阵计算问题；但是KNN、朴素贝叶斯、决策树等等都涉及到比较复杂的递归、迭代以及条件判断。
